{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "named_entity_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHbZE0ftXFGI",
        "outputId": "859fb47a-1a3b-4291-aab1-06e63cdb2d16"
      },
      "source": [
        "!pip install hazm\r\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hazm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.4MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 9.0MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n",
            "Building wheels for collected packages: libwapiti, nltk\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=155187 sha256=a32a429ff8e710bd522eecea5cc1a0124948a7a64f888c3a51c3e89f436d1b38\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394467 sha256=3c33b562d296df5fcbc3d9793946bab5b6cd68a80d30914f9bfc9269e5e9a2a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n",
            "Successfully built libwapiti nltk\n",
            "Installing collected packages: libwapiti, nltk, hazm\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 6.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 19.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=fcff2795ed13a4f81ae4ba349c9795e37523d5ab371f88b2c81fec65e599de9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfSDEbFsMISf"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import hazm\r\n",
        "\r\n",
        "import transformers \r\n",
        "from transformers import AutoTokenizer, AutoConfig\r\n",
        "from transformers import TFAutoModelForTokenClassification\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPCOnKEOXBoX"
      },
      "source": [
        "peyma_translate = {\r\n",
        "    \"B_DAT\": \"تاریخ\",\r\n",
        "    \"B_LOC\": \"موقعیت\",\r\n",
        "    \"B_MON\": \"پول\",\r\n",
        "    \"B_ORG\": \"سازمنان\",\r\n",
        "    \"B_PCT\": \"درصد\",\r\n",
        "    \"B_PER\": \"شخص\",\r\n",
        "    \"B_TIM\": \"زمان\",\r\n",
        "    \"I_DAT\": \"تاریخ\",\r\n",
        "    \"I_LOC\": \"موقعیت\",\r\n",
        "    \"I_MON\": \"پول\",\r\n",
        "    \"I_ORG\": \"سازمان\",\r\n",
        "    \"I_PCT\": \"درصد\",\r\n",
        "    \"I_PER\": \"شخص\",\r\n",
        "    \"I_TIM\": \"زمان\",\r\n",
        "    \"O\": None,\r\n",
        "}\r\n",
        "arman_translate = {\r\n",
        "    \"B-event\": \"رویداد\",\r\n",
        "    \"B-fac\": \"امکانات\",\r\n",
        "    \"B-loc\": \"موقعیت\",\r\n",
        "    \"B-org\": \"سازمان\",\r\n",
        "    \"B-pers\": \"شخص\",\r\n",
        "    \"B-pro\": \"محصول\",\r\n",
        "    \"I-event\": \"رویداد\",\r\n",
        "    \"I-fac\": \"امکانات\",\r\n",
        "    \"I-loc\": \"موقعیت\",\r\n",
        "    \"I-org\": \"سازمان\",\r\n",
        "    \"I-pers\": \"شخص\",\r\n",
        "    \"I-pro\": \"محصول\",\r\n",
        "    \"O\": None\r\n",
        "}\r\n",
        "\r\n",
        "ner_translate = {\r\n",
        "    \"B-date\": \"تاریخ\",\r\n",
        "    \"B-event\": \"رویداد\",\r\n",
        "    \"B-facility\": \"امکانات\",\r\n",
        "    \"B-location\": \"موقعیت\",\r\n",
        "    \"B-money\": \"پول\",\r\n",
        "    \"B-organization\": \"سازمان\",\r\n",
        "    \"B-person\": \"شخص\",\r\n",
        "    \"B-product\": \"محصول\",\r\n",
        "    \"B-time\": \"زمان\",\r\n",
        "    \"B-percent\": \"درصد\",\r\n",
        "    \"I-date\": \"تاریخ\",\r\n",
        "    \"I-event\": \"رویداد\",\r\n",
        "    \"I-facility\": \"امکانات\",\r\n",
        "    \"I-location\": \"موقعیت\",\r\n",
        "    \"I-money\": \"پول\",\r\n",
        "    \"I-organization\": \"سازمان\",\r\n",
        "    \"I-person\": \"شخص\",\r\n",
        "    \"I-product\": \"محصول\",\r\n",
        "    \"I-time\": \"زمان\",\r\n",
        "    \"I-percent\": \"درصد\",\r\n",
        "    \"O\": None\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1VhAXa-XcC7"
      },
      "source": [
        "texts = [\r\n",
        "    \"مدیرکل محیط زیست استان البرز با بیان اینکه با بیان اینکه موضوع شیرابه‌های زباله‌های انتقال یافته در منطقه حلقه دره خطری برای این استان است، گفت: در این مورد گزارشاتی در ۲۵ مرداد ۱۳۹۷ تقدیم مدیران استان شده است.\",\r\n",
        "    \"به گزارش خبرگزاری تسنیم از کرج، حسین محمدی در نشست خبری مشترک با معاون خدمات شهری شهرداری کرج که با حضور مدیرعامل سازمان‌های پسماند، پارک‌ها و فضای سبز و نماینده منابع طبیعی در سالن کنفرانس شهرداری کرج برگزار شد، اظهار داشت: ۸۰٪  جمعیت استان البرز در کلانشهر کرج زندگی می‌کنند.\",\r\n",
        "    \"وی افزود: با همکاری‌های مشترک بین اداره کل محیط زیست و شهرداری کرج برنامه‌های مشترکی برای حفاظت از محیط زیست در شهر کرج در دستور کار قرار گرفته که این اقدامات آثار مثبتی داشته و تاکنون نزدیک به ۱۰۰ میلیارد هزینه جهت خریداری اکس-ریس صورت گرفته است.\",\r\n",
        "]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTnlysRwXm93"
      },
      "source": [
        "\r\n",
        "\r\n",
        "class NER:\r\n",
        "  def __init__(self,model_path):\r\n",
        "    self.normalizer = hazm.Normalizer()\r\n",
        "    self.model, self.tokenizer, self.labels = parsbert_ner_load_model(model_path)\r\n",
        "\r\n",
        "  def cleanize(self,text):\r\n",
        "      \"\"\"A way to normalize and even clean the text\"\"\"\r\n",
        "      # clean text\r\n",
        "      # do some fns\r\n",
        "      return self.normalizer.normalize(text)\r\n",
        "\r\n",
        "\r\n",
        "  def parsbert_ner_load_model(model_name):\r\n",
        "      \"\"\"Load the model\"\"\"\r\n",
        "      try:\r\n",
        "          config = AutoConfig.from_pretrained(model_name)\r\n",
        "          tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "          model = TFAutoModelForTokenClassification.from_pretrained(model_name)\r\n",
        "          labels = list(config.label2id.keys())\r\n",
        "          return model, tokenizer, labels\r\n",
        "      except:\r\n",
        "          return [None] * 3\r\n",
        "\r\n",
        "\r\n",
        "  def predict(self,texts):\r\n",
        "      output_predictions = []\r\n",
        "      for sequence in texts:\r\n",
        "          sequence = self.cleanize(sequence)\r\n",
        "          tokens = self.tokenizer.tokenize(self.tokenizer.decode(self.tokenizer.encode(sequence)))\r\n",
        "          inputs = self.tokenizer.encode(sequence, return_tensors=\"tf\")\r\n",
        "          outputs = self.model(inputs)[0]\r\n",
        "          predictions = tf.argmax(outputs, axis=2)\r\n",
        "          predictions = [(token, self.labels[prediction]) for token, prediction in zip(tokens, predictions[0].numpy())]\r\n",
        "          output_predictions.append(predictions)\r\n",
        "\r\n",
        "      return output_predictions\r\n",
        "    \r\n",
        "  def predict_filtered(self,texts):\r\n",
        "    output_predictions = self.predict(texts)\r\n",
        "    return [[pair for pair in array if pair[1] != 'O'] for array in output_predictions]\r\n",
        "\r\n",
        "  def predict_concat(self,texts):\r\n",
        "      output_filtered = self.predict_filtered(texts)\r\n",
        "      result = []\r\n",
        "      for array in output_filtered:\r\n",
        "        tokens_concat = []\r\n",
        "        current_token = ''\r\n",
        "        current_cat = ''\r\n",
        "        for token,cat in array:\r\n",
        "          if cat[0] == 'B':\r\n",
        "            if current_token != '': tokens_concat.append((current_token,current_cat))\r\n",
        "            current_token = token\r\n",
        "            current_cat = cat[2:]\r\n",
        "          elif cat[0] == 'I':\r\n",
        "            current_token = current_token + ' '+ token\r\n",
        "        if current_token != '': tokens_concat.append((current_token,current_cat))\r\n",
        "        result.append(tokens_concat)\r\n",
        "      return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acdW_Ll8ZWAh",
        "outputId": "ec714a62-aa80-44ba-d392-b092f1062b1b"
      },
      "source": [
        "#initializing ner class\r\n",
        "ner = NER('HooshvareLab/bert-base-parsbert-ner-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased were not used when initializing TFBertForTokenClassification: ['dropout_37']\n",
            "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at HooshvareLab/bert-base-parsbert-ner-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbNFD6YlZ6G-",
        "outputId": "e1177bf1-9818-424e-fd81-60dcd3d77b72"
      },
      "source": [
        "ner.predict_concat(texts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('محیط زیست استان البرز', 'organization'),\n",
              "  ('منطقه حلقه دره', 'location'),\n",
              "  ('۲۵ مرداد ۱۳۹۷', 'date')],\n",
              " [('خبرگزاری تسنیم', 'organization'),\n",
              "  ('کرج', 'location'),\n",
              "  ('حسین محمدی', 'person'),\n",
              "  ('شهرداری کرج', 'organization'),\n",
              "  ('سازمانهای پسماند ، پارکها و فضای سبز', 'organization'),\n",
              "  ('منابع طبیعی', 'organization'),\n",
              "  ('سالن کنفرانس شهرداری کرج', 'location'),\n",
              "  ('۸۰ ٪', 'percent'),\n",
              "  ('استان البرز', 'location'),\n",
              "  ('کرج', 'location')],\n",
              " [('اداره کل محیط زیست', 'organization'),\n",
              "  ('شهرداری کرج', 'organization'),\n",
              "  ('شهر کرج', 'location'),\n",
              "  ('۱۰۰ میلیارد', 'money')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfap_M9dap8Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}